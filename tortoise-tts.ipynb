{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "509b30bf-04fb-4c9b-8479-43e0d99a51d1",
   "metadata": {},
   "source": [
    "Install tortoise-tts using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e30e7da-ab13-4719-91b1-07693643fbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/neonbjb/tortoise-tts\n",
      "  Cloning https://github.com/neonbjb/tortoise-tts to /tmp/pip-req-build-5vldyt5j\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neonbjb/tortoise-tts /tmp/pip-req-build-5vldyt5j\n",
      "  Resolved https://github.com/neonbjb/tortoise-tts to commit 1a3b014d5c5b14feaa416145004411a7ee2a3970\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops in ./venv/lib/python3.10/site-packages (from tortoise-tts==3.0.0) (0.7.0)\n",
      "Requirement already satisfied: inflect in ./venv/lib/python3.10/site-packages (from tortoise-tts==3.0.0) (7.2.0)\n",
      "Requirement already satisfied: librosa in ./venv/lib/python3.10/site-packages (from tortoise-tts==3.0.0) (0.10.1)\n",
      "Requirement already satisfied: progressbar in ./venv/lib/python3.10/site-packages (from tortoise-tts==3.0.0) (2.5)\n",
      "Requirement already satisfied: rotary_embedding_torch in ./venv/lib/python3.10/site-packages (from tortoise-tts==3.0.0) (0.5.3)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.10/site-packages (from tortoise-tts==3.0.0) (1.13.0)\n",
      "Requirement already satisfied: tokenizers in ./venv/lib/python3.10/site-packages (from tortoise-tts==3.0.0) (0.13.3)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from tortoise-tts==3.0.0) (4.66.2)\n",
      "Requirement already satisfied: transformers==4.31.0 in ./venv/lib/python3.10/site-packages (from tortoise-tts==3.0.0) (4.31.0)\n",
      "Requirement already satisfied: unidecode in ./venv/lib/python3.10/site-packages (from tortoise-tts==3.0.0) (1.3.8)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers==4.31.0->tortoise-tts==3.0.0) (2024.4.16)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from transformers==4.31.0->tortoise-tts==3.0.0) (24.0)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from transformers==4.31.0->tortoise-tts==3.0.0) (2.31.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from transformers==4.31.0->tortoise-tts==3.0.0) (3.13.4)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from transformers==4.31.0->tortoise-tts==3.0.0) (1.26.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./venv/lib/python3.10/site-packages (from transformers==4.31.0->tortoise-tts==3.0.0) (0.4.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from transformers==4.31.0->tortoise-tts==3.0.0) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in ./venv/lib/python3.10/site-packages (from transformers==4.31.0->tortoise-tts==3.0.0) (0.22.2)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in ./venv/lib/python3.10/site-packages (from inflect->tortoise-tts==3.0.0) (4.2.1)\n",
      "Requirement already satisfied: more-itertools in ./venv/lib/python3.10/site-packages (from inflect->tortoise-tts==3.0.0) (10.2.0)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.10/site-packages (from inflect->tortoise-tts==3.0.0) (4.11.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in ./venv/lib/python3.10/site-packages (from librosa->tortoise-tts==3.0.0) (0.4)\n",
      "Requirement already satisfied: pooch>=1.0 in ./venv/lib/python3.10/site-packages (from librosa->tortoise-tts==3.0.0) (1.8.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./venv/lib/python3.10/site-packages (from librosa->tortoise-tts==3.0.0) (0.59.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in ./venv/lib/python3.10/site-packages (from librosa->tortoise-tts==3.0.0) (1.4.2)\n",
      "Requirement already satisfied: msgpack>=1.0 in ./venv/lib/python3.10/site-packages (from librosa->tortoise-tts==3.0.0) (1.0.8)\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./venv/lib/python3.10/site-packages (from librosa->tortoise-tts==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in ./venv/lib/python3.10/site-packages (from librosa->tortoise-tts==3.0.0) (0.12.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./venv/lib/python3.10/site-packages (from librosa->tortoise-tts==3.0.0) (5.1.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./venv/lib/python3.10/site-packages (from librosa->tortoise-tts==3.0.0) (0.3.7)\n",
      "Requirement already satisfied: joblib>=0.14 in ./venv/lib/python3.10/site-packages (from librosa->tortoise-tts==3.0.0) (1.4.0)\n",
      "Requirement already satisfied: torch>=2.0 in ./venv/lib/python3.10/site-packages (from rotary_embedding_torch->tortoise-tts==3.0.0) (2.2.2)\n",
      "Requirement already satisfied: beartype in ./venv/lib/python3.10/site-packages (from rotary_embedding_torch->tortoise-tts==3.0.0) (0.18.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->tortoise-tts==3.0.0) (2024.3.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in ./venv/lib/python3.10/site-packages (from numba>=0.51.0->librosa->tortoise-tts==3.0.0) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./venv/lib/python3.10/site-packages (from pooch>=1.0->librosa->tortoise-tts==3.0.0) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->transformers==4.31.0->tortoise-tts==3.0.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->transformers==4.31.0->tortoise-tts==3.0.0) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->transformers==4.31.0->tortoise-tts==3.0.0) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->transformers==4.31.0->tortoise-tts==3.0.0) (2.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa->tortoise-tts==3.0.0) (3.4.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./venv/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa->tortoise-tts==3.0.0) (1.16.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./venv/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./venv/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (12.1.0.106)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in ./venv/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (2.2.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (12.1.105)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (1.12)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (11.0.2.54)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (3.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (12.4.127)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->tortoise-tts==3.0.0) (2.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/neonbjb/tortoise-tts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6763851d-57b9-4ac1-970f-ff4fd935afca",
   "metadata": {},
   "source": [
    "Confirm the installation was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95bc58ef-5460-4408-a1cf-77ebaa391d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/djh/Development/python/tortoise-tts/venv/bin/tortoise_tts.py\", line 10, in <module>\n",
      "    import torchaudio\n",
      "ModuleNotFoundError: No module named 'torchaudio'\n"
     ]
    }
   ],
   "source": [
    "!tortoise_tts.py --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f1cdf-4ee1-4032-85ab-51b739295fc4",
   "metadata": {},
   "source": [
    "The torchaudio package appears to be missing from requirements.txt. Let's install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5836cbbe-1616-4620-951c-db59f4f8e63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchaudio in ./venv/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: torch==2.2.2 in ./venv/lib/python3.10/site-packages (from torchaudio) (2.2.2)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (1.12)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (11.4.5.107)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (12.1.0.106)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (4.11.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (2.19.3)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (3.13.4)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (10.3.2.106)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchaudio) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch==2.2.2->torchaudio) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->torch==2.2.2->torchaudio) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "053614bb-854f-4760-bd4b-f55197826bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: tortoise_tts.py [-h] [-v, --voice VOICE] [-V, --voices-dir VOICES_DIR]\n",
      "                       [-p, --preset {ultra_fast,fast,standard,high_quality}]\n",
      "                       [-q, --quiet]\n",
      "                       (-l, --list-voices | -P, --play | -o, --output OUTPUT | -O, --output-dir OUTPUT_DIR)\n",
      "                       [--candidates CANDIDATES] [--regenerate REGENERATE]\n",
      "                       [--skip-existing] [--produce-debug-state] [--seed SEED]\n",
      "                       [--models-dir MODELS_DIR] [--text-split TEXT_SPLIT]\n",
      "                       [--disable-redaction] [--device DEVICE]\n",
      "                       [--batch-size BATCH_SIZE]\n",
      "                       [--num-autoregressive-samples NUM_AUTOREGRESSIVE_SAMPLES]\n",
      "                       [--temperature TEMPERATURE]\n",
      "                       [--length-penalty LENGTH_PENALTY]\n",
      "                       [--repetition-penalty REPETITION_PENALTY]\n",
      "                       [--top-p TOP_P] [--max-mel-tokens MAX_MEL_TOKENS]\n",
      "                       [--cvvp-amount CVVP_AMOUNT]\n",
      "                       [--diffusion-iterations DIFFUSION_ITERATIONS]\n",
      "                       [--cond-free COND_FREE] [--cond-free-k COND_FREE_K]\n",
      "                       [--diffusion-temperature DIFFUSION_TEMPERATURE]\n",
      "                       [text ...]\n",
      "tortoise_tts.py: error: one of the arguments -l, --list-voices -P, --play -o, --output -O, --output-dir is required\n"
     ]
    }
   ],
   "source": [
    "!tortoise_tts.py --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d1fae-bb43-455d-a0b6-bc5fe0cddd49",
   "metadata": {},
   "source": [
    "Let's see what voices we can choose from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56f7049-3a77-4743-92ae-0a9f307bb1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: tortoise_tts.py [-h] [-v, --voice VOICE] [-V, --voices-dir VOICES_DIR]\n",
      "                       [-p, --preset {ultra_fast,fast,standard,high_quality}]\n",
      "                       [-q, --quiet]\n",
      "                       (-l, --list-voices | -P, --play | -o, --output OUTPUT | -O, --output-dir OUTPUT_DIR)\n",
      "                       [--candidates CANDIDATES] [--regenerate REGENERATE]\n",
      "                       [--skip-existing] [--produce-debug-state] [--seed SEED]\n",
      "                       [--models-dir MODELS_DIR] [--text-split TEXT_SPLIT]\n",
      "                       [--disable-redaction] [--device DEVICE]\n",
      "                       [--batch-size BATCH_SIZE]\n",
      "                       [--num-autoregressive-samples NUM_AUTOREGRESSIVE_SAMPLES]\n",
      "                       [--temperature TEMPERATURE]\n",
      "                       [--length-penalty LENGTH_PENALTY]\n",
      "                       [--repetition-penalty REPETITION_PENALTY]\n",
      "                       [--top-p TOP_P] [--max-mel-tokens MAX_MEL_TOKENS]\n",
      "                       [--cvvp-amount CVVP_AMOUNT]\n",
      "                       [--diffusion-iterations DIFFUSION_ITERATIONS]\n",
      "                       [--cond-free COND_FREE] [--cond-free-k COND_FREE_K]\n",
      "                       [--diffusion-temperature DIFFUSION_TEMPERATURE]\n",
      "                       [text ...]\n",
      "tortoise_tts.py: error: one of the arguments -l, --list-voices -P, --play -o, --output -O, --output-dir is required\n"
     ]
    }
   ],
   "source": [
    "!tortoise_tts.py --list-voices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d83381e-2894-49e2-8624-1306973dc124",
   "metadata": {},
   "source": [
    "That doesn't seem to be a valid option. Let's try another approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae994914-0631-4b25-9ab6-f383e04971d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angie\n",
      "applejack\n",
      "cond_latent_example\n",
      "daniel\n",
      "deniro\n",
      "emma\n",
      "freeman\n",
      "geralt\n",
      "halle\n",
      "jlaw\n",
      "lj\n",
      "mol\n",
      "myself\n",
      "pat\n",
      "pat2\n",
      "rainbow\n",
      "snakes\n",
      "tim_reynolds\n",
      "tom\n",
      "train_atkins\n",
      "train_daws\n",
      "train_dotrice\n",
      "train_dreams\n",
      "train_empire\n",
      "train_grace\n",
      "train_kennard\n",
      "train_lescault\n",
      "train_mouse\n",
      "weaver\n",
      "william\n"
     ]
    }
   ],
   "source": [
    "!tortoise_tts.py -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a17cc-6e70-4df3-9e2c-f3a01cf89935",
   "metadata": {},
   "source": [
    "That worked as expexted. Let's create a test speech sample using the prerecorded voice `tom` with a quality preset of `ultra_fast`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ba9961f-6fe5-401a-b337-536f48019b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Rendering tom_00 (1 of 1)...\n",
      "  Testing, testing, one two, one two three.\n",
      "Generating autoregressive samples..\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:08<00:00,  8.66s/it]\n",
      "Computing best candidates using CLVP\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  2.53it/s]\n",
      "Transforming autoregressive outputs into audio..\n",
      "100%|███████████████████████████████████████████| 30/30 [00:00<00:00, 83.60it/s]\n",
      "\n",
      "real\t0m23.369s\n",
      "user\t0m32.982s\n",
      "sys\t0m5.874s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 42 -p ultra_fast -v tom -o tom-ultra-fast-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfe9329-a7cb-4f3c-8d0b-e7c04133f8c0",
   "metadata": {},
   "source": [
    "The recording sound quality isn't great, but hey it's the `ultra_fast` preset option. Let's see how good the `high_quality` preset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6e0d1e2-b8cb-4c7d-84bc-7cf8f518aacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Rendering tom_00 (1 of 1)...\n",
      "  Testing, testing, one two, one two three.\n",
      "Generating autoregressive samples..\n",
      "100%|███████████████████████████████████████████| 16/16 [02:12<00:00,  8.27s/it]\n",
      "Computing best candidates using CLVP\n",
      "100%|███████████████████████████████████████████| 16/16 [00:05<00:00,  2.68it/s]\n",
      "Transforming autoregressive outputs into audio..\n",
      "100%|█████████████████████████████████████████| 400/400 [00:09<00:00, 41.68it/s]\n",
      "\n",
      "real\t2m43.285s\n",
      "user\t2m50.969s\n",
      "sys\t0m6.639s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 42 -p high_quality -v tom -o tom-high-quality-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e532dd7d-719a-412e-8cb2-bea5bbe01fd7",
   "metadata": {},
   "source": [
    "The quality has improved but at the cost of six times the GPU processing compared with using the `ultra_fast` preset. It requires approximately 30 seconds of GPU time per word to create a high fidelity output. As the author mentions, it's called tortoise for a good reason. Let's see if we can improve performance without losing too much quality.\n",
    "\n",
    "Let's first listen to the audio quality of of the `standard` preset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ca33c8b-9734-4e59-ac3a-fabf76de0967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Rendering tom_00 (1 of 1)...\n",
      "  Testing, testing, one two, one two three.\n",
      "Generating autoregressive samples..\n",
      "100%|███████████████████████████████████████████| 16/16 [02:12<00:00,  8.27s/it]\n",
      "Computing best candidates using CLVP\n",
      "100%|███████████████████████████████████████████| 16/16 [00:05<00:00,  2.68it/s]\n",
      "Transforming autoregressive outputs into audio..\n",
      "100%|█████████████████████████████████████████| 200/200 [00:04<00:00, 41.63it/s]\n",
      "\n",
      "real\t2m37.722s\n",
      "user\t2m46.332s\n",
      "sys\t0m6.233s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 42 -p standard -v tom -o tom-standard-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109f114-d8fe-4eb9-afa8-3e61109088cd",
   "metadata": {},
   "source": [
    "I cannot hear a significant difference in the quality of the `standard` vs `high_quality` presents. The GPU time required by each is similar for this short test. Let's explore difference options starting with caching.\n",
    "\n",
    "Tortoise include support for `kv_caching`. Instead of recomputing the key matrix (all the previous context the model should pay attention to) and value matrix (weighted sum over previous context) in the decoder, we can cache results. The k and v matrices don't change very much as we output new tokens. Refer to this [video](https://youtu.be/80bIUggRJf4?si=ceCOeyQlhFY9AiCD) for a helpful explanation.\n",
    "\n",
    "Unfortunately, the command line interface doesn't support a kv_cache input option so let's update `tortoise_tts.py` to enable it. Change line 209 of `venv/bin/tortoise_tts.py` to:\n",
    "\n",
    "```python\n",
    "tts = TextToSpeech(kv_cache=True, models_dir=args.models_dir, enable_redaction=not args.disable_redaction,\n",
    "\t\t\t\t   device=args.device, autoregressive_batch_size=args.batch_size)\n",
    "```\n",
    "\n",
    "and rerun the earlier test using the `high_quality` preset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba8cd07-b41b-4410-88a5-5c26fcdb69e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Rendering tom_00 (1 of 1)...\n",
      "  Testing, testing, one two, one two three.\n",
      "Generating autoregressive samples..\n",
      "100%|███████████████████████████████████████████| 16/16 [00:14<00:00,  1.08it/s]\n",
      "Computing best candidates using CLVP\n",
      "100%|███████████████████████████████████████████| 16/16 [00:05<00:00,  2.72it/s]\n",
      "Transforming autoregressive outputs into audio..\n",
      "100%|█████████████████████████████████████████| 400/400 [00:09<00:00, 41.36it/s]\n",
      "\n",
      "real\t0m45.929s\n",
      "user\t0m53.813s\n",
      "sys\t0m5.994s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 42 -p high_quality -v tom -o tom-high-quality-kvcache-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c573a745-0514-46a2-adbc-4a80700cff94",
   "metadata": {},
   "source": [
    "Enabling `kv_cache` resulted in a significant improvement in performance. We've reduced the time taken by a factor of four but the audio fidelity sounds different. This is unexpected. I used the same `seed` value across tests so if `kv_cache` is indeed avoiding unnecessary computations of k and v values, I wouldn't expect a difference in the output wav file generated. It's difficult to objectively say that the new audio is worse without scientifically comparing the two segments, but qualitatively the latest recording doesn't sound as good.\n",
    "\n",
    "It looks like this is a known issue with the origial source code. It's been addressed in this derivative repo [tortoise-tts-fast](https://github.com/152334H/tortoise-tts-fast). Explore this later as the derivative code base uses a new sampler `dpm++2m`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6f6fe5-17e0-4d5b-a096-ff561a8ab1fb",
   "metadata": {},
   "source": [
    "Tortoise-tts also support the deepspeed library from Microsoft. One of the four innovation pillars its development focusses on is Inference of LLMs. One again we'll need to enable its use by modifying `tortoise_tts.py`. Change line 209 of venv/bin/tortoise_tts.py to:\n",
    "\n",
    "```python\n",
    "tts = TextToSpeech(use_deepspeed=True, models_dir=args.models_dir, enable_redaction=not args.disable_redaction,\n",
    "                   device=args.device, autoregressive_batch_size=args.batch_size)\n",
    "```                   \n",
    "and rerun the earlier test using the high_quality preset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046e7d4-3571-4d93-bcbf-ab77138a79fb",
   "metadata": {},
   "source": [
    "> [!NOTE]  \n",
    "> I've removed the earlier setting of the argument `kv_cache` so we are only changing one variable at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004260ce-a4ff-41d5-9a53-c20b3a730874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djh/Development/python/tortoise-tts/venv/bin/tortoise_tts.py\", line 209, in <module>\n",
      "    tts = TextToSpeech(use_deepspeed=True, models_dir=args.models_dir, enable_redaction=not args.disable_redaction,\n",
      "  File \"/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/tortoise/api.py\", line 218, in __init__\n",
      "    self.autoregressive.post_init_gpt2_config(use_deepspeed=use_deepspeed, kv_cache=kv_cache, half=self.half)\n",
      "  File \"/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/tortoise/models/autoregressive.py\", line 387, in post_init_gpt2_config\n",
      "    import deepspeed\n",
      "ModuleNotFoundError: No module named 'deepspeed'\n",
      "\n",
      "real\t0m8.923s\n",
      "user\t0m12.962s\n",
      "sys\t0m2.652s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 42 -p high_quality -v tom -o tom-high-quality-deepspeed-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd75adab-7116-4e49-a6cd-48f80a10766c",
   "metadata": {},
   "source": [
    "The module is mission so we'll need to install it and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea47078d-0e24-442a-91ae-949f3a572a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepspeed in ./venv/lib/python3.10/site-packages (0.14.1)\n",
      "Requirement already satisfied: hjson in ./venv/lib/python3.10/site-packages (from deepspeed) (3.1.0)\n",
      "Requirement already satisfied: ninja in ./venv/lib/python3.10/site-packages (from deepspeed) (1.11.1.1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (from deepspeed) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from deepspeed) (24.0)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.10/site-packages (from deepspeed) (5.9.8)\n",
      "Requirement already satisfied: py-cpuinfo in ./venv/lib/python3.10/site-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic in ./venv/lib/python3.10/site-packages (from deepspeed) (2.7.0)\n",
      "Requirement already satisfied: pynvml in ./venv/lib/python3.10/site-packages (from deepspeed) (11.5.0)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.10/site-packages (from deepspeed) (2.2.2)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from deepspeed) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./venv/lib/python3.10/site-packages (from pydantic->deepspeed) (4.11.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.10/site-packages (from pydantic->deepspeed) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in ./venv/lib/python3.10/site-packages (from pydantic->deepspeed) (2.18.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (3.13.4)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (3.1.3)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (3.3)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (2.2.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (11.4.5.107)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (1.12)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "492bd41b-04c7-48c7-b49d-42886d8d0a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2024-04-19 12:53:38,898] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n",
      "[2024-04-19 12:53:39,147] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.1, git-hash=unknown, git-branch=unknown\n",
      "[2024-04-19 12:53:39,147] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2024-04-19 12:53:39,147] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "Using /home/djh/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/djh/.cache/torch_extensions/py310_cu121/transformer_inference/build.ninja...\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output pointwise_ops.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pointwise_ops.cu -o pointwise_ops.cuda.o \n",
      "[2/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output relu.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/relu.cu -o relu.cuda.o \n",
      "[3/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output dequantize.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/dequantize.cu -o dequantize.cuda.o \n",
      "[4/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output transform.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu -o transform.cuda.o \n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(38): warning #177-D: variable \"d0_stride\" was declared but never referenced\n",
      "      int d0_stride = hidden_dim * seq_length;\n",
      "          ^\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(66): warning #177-D: variable \"lane\" was declared but never referenced\n",
      "      int lane = d3 & 0x1f;\n",
      "          ^\n",
      "\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(109): warning #177-D: variable \"half_dim\" was declared but never referenced\n",
      "      unsigned half_dim = (rotary_dim << 3) >> 1;\n",
      "               ^\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int, float) [with T=__nv_bfloat16]\" at line 281\n",
      "\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(110): warning #177-D: variable \"d0_stride\" was declared but never referenced\n",
      "      int d0_stride = hidden_dim * seq_length;\n",
      "          ^\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int, float) [with T=__nv_bfloat16]\" at line 281\n",
      "\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(126): warning #177-D: variable \"vals_half\" was declared but never referenced\n",
      "      T2* vals_half = reinterpret_cast<T2*>(&vals_arr);\n",
      "          ^\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int, float) [with T=__nv_bfloat16]\" at line 281\n",
      "\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(127): warning #177-D: variable \"output_half\" was declared but never referenced\n",
      "      T2* output_half = reinterpret_cast<T2*>(&output_arr);\n",
      "          ^\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int, float) [with T=__nv_bfloat16]\" at line 281\n",
      "\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(144): warning #177-D: variable \"lane\" was declared but never referenced\n",
      "      int lane = d3 & 0x1f;\n",
      "          ^\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int, float) [with T=__nv_bfloat16]\" at line 281\n",
      "\n",
      "[5/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output gelu.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/gelu.cu -o gelu.cuda.o \n",
      "[6/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output apply_rotary_pos_emb.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu -o apply_rotary_pos_emb.cuda.o \n",
      "[7/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output rms_norm.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/rms_norm.cu -o rms_norm.cuda.o \n",
      "[8/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output softmax.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/softmax.cu -o softmax.cuda.o \n",
      "[9/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output layer_norm.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/layer_norm.cu -o layer_norm.cuda.o \n",
      "[10/11] c++ -MMD -MF pt_binding.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DBF16_AVAILABLE -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp -o pt_binding.o \n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_softmax_context(at::Tensor&, at::Tensor&, int, bool, bool, int, int, float, bool, bool, int, bool, unsigned int, unsigned int, at::Tensor&, float) [with T = float]’:\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:2015:5:   required from here\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:541:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  541 |                                      {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:541:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:542:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  542 |                                       k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:542:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:550:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  550 |                          {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:550:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:551:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  551 |                           k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:551:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_rms_mlp_gemm(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, float, at::Tensor&, at::Tensor&, bool, int, bool) [with T = float]’:\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:2015:5:   required from here\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1581:72: warning: narrowing conversion of ‘(size_t)mlp_1_out_neurons’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      " 1581 |         at::from_blob(intermediate_ptr, {input.size(0), input.size(1), mlp_1_out_neurons}, options);\n",
      "      |                                                                        ^~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1581:72: warning: narrowing conversion of ‘mlp_1_out_neurons’ from ‘const size_t’ {aka ‘const long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_softmax_context(at::Tensor&, at::Tensor&, int, bool, bool, int, int, float, bool, bool, int, bool, unsigned int, unsigned int, at::Tensor&, float) [with T = __half]’:\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:2016:5:   required from here\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:541:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  541 |                                      {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:541:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:542:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  542 |                                       k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:542:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:550:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  550 |                          {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:550:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:551:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  551 |                           k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:551:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_rms_mlp_gemm(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, float, at::Tensor&, at::Tensor&, bool, int, bool) [with T = __half]’:\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:2016:5:   required from here\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1581:72: warning: narrowing conversion of ‘(size_t)mlp_1_out_neurons’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      " 1581 |         at::from_blob(intermediate_ptr, {input.size(0), input.size(1), mlp_1_out_neurons}, options);\n",
      "      |                                                                        ^~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1581:72: warning: narrowing conversion of ‘mlp_1_out_neurons’ from ‘const size_t’ {aka ‘const long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_softmax_context(at::Tensor&, at::Tensor&, int, bool, bool, int, int, float, bool, bool, int, bool, unsigned int, unsigned int, at::Tensor&, float) [with T = __nv_bfloat16]’:\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:2018:5:   required from here\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:541:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  541 |                                      {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:541:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:542:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  542 |                                       k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:542:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:550:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  550 |                          {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:550:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:551:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  551 |                           k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:551:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_rms_mlp_gemm(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, float, at::Tensor&, at::Tensor&, bool, int, bool) [with T = __nv_bfloat16]’:\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:2018:5:   required from here\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1581:72: warning: narrowing conversion of ‘(size_t)mlp_1_out_neurons’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      " 1581 |         at::from_blob(intermediate_ptr, {input.size(0), input.size(1), mlp_1_out_neurons}, options);\n",
      "      |                                                                        ^~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1581:72: warning: narrowing conversion of ‘mlp_1_out_neurons’ from ‘const size_t’ {aka ‘const long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "[11/11] c++ pt_binding.o gelu.cuda.o relu.cuda.o layer_norm.cuda.o rms_norm.cuda.o softmax.cuda.o dequantize.cuda.o apply_rotary_pos_emb.cuda.o transform.cuda.o pointwise_ops.cuda.o -shared -lcurand -L/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o transformer_inference.so\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 15.73634123802185 seconds\n",
      "[2024-04-19 12:53:54,895] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000, 'invert_mask': True}\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Rendering tom_00 (1 of 1)...\n",
      "  Testing, testing, one two, one two three.\n",
      "Generating autoregressive samples..\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]------------------------------------------------------\n",
      "Free memory : 17.104248 (GigaBytes)  \n",
      "Total memory: 19.681274 (GigaBytes)  \n",
      "Requested memory: 5.375000 (GigaBytes) \n",
      "Setting maximum total tokens (input + output) to 1024 \n",
      "WorkSpace: 0x7f4678000000 \n",
      "------------------------------------------------------\n",
      "100%|███████████████████████████████████████████| 16/16 [00:42<00:00,  2.64s/it]\n",
      "Computing best candidates using CLVP\n",
      "100%|███████████████████████████████████████████| 16/16 [00:05<00:00,  2.69it/s]\n",
      "Transforming autoregressive outputs into audio..\n",
      "100%|█████████████████████████████████████████| 400/400 [00:09<00:00, 42.00it/s]\n",
      "\n",
      "real\t1m28.277s\n",
      "user\t1m59.131s\n",
      "sys\t0m9.289s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 42 -p high_quality -v tom -o tom-high-quality-deepspeed-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea1a61-f93f-4a40-8740-296ea012eae4",
   "metadata": {},
   "source": [
    "If this fails and complains of missing header files you may need to install the development python package for the version of python you are using in your vitual environment.\n",
    "\n",
    "```bash\n",
    "sudo apt-get install python3.10-dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4f849-87be-42c5-9978-8300438a567c",
   "metadata": {},
   "source": [
    "The deepspeed module reduced the GPU time by 50%. There was no audible reduction in the fidelity of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9056c688-4e90-42e1-ab65-1812fff63b67",
   "metadata": {},
   "source": [
    "The last performance option we'll explore is using half-precision 16bit floats. Again we need to modify the code:\n",
    "\n",
    "```python\n",
    "tts = TextToSpeech(half=True, models_dir=args.models_dir, enable_redaction=not args.disable_redaction,\n",
    "                   device=args.device, autoregressive_batch_size=args.batch_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27d5e21c-8efe-462e-973a-3e8f79760ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Rendering tom_00 (1 of 1)...\n",
      "  Testing, testing, one two, one two three.\n",
      "Generating autoregressive samples..\n",
      "100%|███████████████████████████████████████████| 16/16 [00:53<00:00,  3.34s/it]\n",
      "Computing best candidates using CLVP\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  5.37it/s]\n",
      "Transforming autoregressive outputs into audio..\n",
      "100%|█████████████████████████████████████████| 400/400 [00:09<00:00, 41.48it/s]\n",
      "\n",
      "real\t1m20.510s\n",
      "user\t1m29.539s\n",
      "sys\t0m5.881s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 42 -p high_quality -v tom -o tom-high-quality-halfp-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee807d-1d19-4f62-9b64-d52e56c4ac2c",
   "metadata": {},
   "source": [
    "Similar to using the deepspeed module, using half precision resulted in a 50% reduction in GPU time. While there's no obvious reduction in audio fidelity, I would expect a quantitative comparison to show there is a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aadb69-03e5-48fb-9e66-5e6dcbbdc144",
   "metadata": {},
   "source": [
    "We can enable all three options at once, thereby resulting in a significant improvement in performance. Again we modify the code:\n",
    "\n",
    "```python\n",
    "tts = TextToSpeech(kv_cache=True, use_deepspeed=True, half=True, models_dir=args.models_dir, enable_redaction=not args.disable_redaction,\n",
    "                   device=args.device, autoregressive_batch_size=args.batch_size)\n",
    "```\n",
    "\n",
    "and rerun the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "364a24d7-efc5-44d5-9778-6689de791b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2024-04-19 13:19:09,928] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n",
      "[2024-04-19 13:19:10,177] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.1, git-hash=unknown, git-branch=unknown\n",
      "[2024-04-19 13:19:10,178] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2024-04-19 13:19:10,178] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "Using /home/djh/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/djh/.cache/torch_extensions/py310_cu121/transformer_inference/build.ninja...\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.029194355010986328 seconds\n",
      "[2024-04-19 13:19:10,258] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float16, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000, 'invert_mask': True}\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Rendering tom_00 (1 of 1)...\n",
      "  Testing, testing, one two, one two three.\n",
      "Generating autoregressive samples..\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]------------------------------------------------------\n",
      "Free memory : 17.815186 (GigaBytes)  \n",
      "Total memory: 19.681274 (GigaBytes)  \n",
      "Requested memory: 5.375000 (GigaBytes) \n",
      "Setting maximum total tokens (input + output) to 1024 \n",
      "WorkSpace: 0x7f235c000000 \n",
      "------------------------------------------------------\n",
      "100%|███████████████████████████████████████████| 16/16 [00:06<00:00,  2.43it/s]\n",
      "Computing best candidates using CLVP\n",
      "100%|███████████████████████████████████████████| 16/16 [00:02<00:00,  5.43it/s]\n",
      "Transforming autoregressive outputs into audio..\n",
      "100%|█████████████████████████████████████████| 400/400 [00:08<00:00, 45.15it/s]\n",
      "\n",
      "real\t0m32.573s\n",
      "user\t0m42.285s\n",
      "sys\t0m5.633s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 42 -p high_quality -v tom -o tom-high-quality-halfp-deep-cache-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb17374-bdcc-4087-a460-4fff4a09d565",
   "metadata": {},
   "source": [
    "We've reduced GPU time by approximately 85% without any notable reduction in the audible fidelity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca3714-18cc-4001-bc37-593cd20eb4b9",
   "metadata": {},
   "source": [
    "I've included each of the synthetically generated text-to-speech audio files under the folder /output-audio-samples should you wish to listen and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e3b0c-e8f2-4acf-95e1-bc65e8833500",
   "metadata": {},
   "source": [
    "Let's now revist the earlier fork of the original repo.\n",
    "\n",
    "- https://github.com/neonbjb/tortoise-tts ↩\n",
    "    - https://github.com/152334H/tortoise-tts-fast/forks ↩\n",
    "        - https://github.com/manmay-nakhashi/tortoise-tts-fastest\n",
    "\n",
    "Active development seems to have stopped, well at least that's what appears to be the case based on when the forks were last updated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821ba4c-7de7-4a70-973d-715ecba82930",
   "metadata": {},
   "source": [
    "Let's switch to this fork and re-establish a baseline. There appears to be incompatibilities between https://github.com/manmay-nakhashi/tortoise-tts-fastest and python3.10. The repo contains a colab notebook configured for python3.8. This is a potential future side project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcecf6b-deb7-4c7f-a682-6ffd3ec63c78",
   "metadata": {},
   "source": [
    "## Creating a Custom Voice\n",
    "\n",
    "The process of creating a new custom voice appears to be quite simple. First we capture five to ten examples of the voice we wish to copy. Each recording should be no more than ten seconds long. I asked Google's Gemini for ten training texts. It provided the following:\n",
    "\n",
    "1. **Historical Fact:** The Great Wall of China is the longest man-made structure in the world, stretching over 21,000 kilometers. (9 seconds)\n",
    "2. **Instructional:** When crossing the street, look both ways before stepping off the curb. (8 seconds)\n",
    "3. **Technical Description:** A laptop computer is a portable personal computer designed for mobile use. (9 seconds)\n",
    "4. **News Report:** Local libraries will be offering a series of free workshops on creative writing throughout the summer. (10 seconds)\n",
    "5. **Restaurant Order:** I would like the chicken stir-fry with brown rice and a side salad, please. (8 seconds)\n",
    "6. **Travel Description:** The bustling marketplace was filled with vendors selling colorful fabrics and handcrafted souvenirs. (10 seconds)\n",
    "7. **Animal Fact:** The average lifespan of a domestic cat is around 15 years. (8 seconds)\n",
    "8. **Book Description:** This science fiction novel explores the concept of faster-than-light travel. (9 seconds)\n",
    "9. **Daily Routine:** I usually wake up at 7 am, exercise for 30 minutes, and then head to work. (9 seconds)\n",
    "10. **Movie Summary:** The heartwarming film follows a group of misfits who come together to achieve a common goal. (9 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef203cd-8cb6-4727-a04f-0dd7078c7aa6",
   "metadata": {},
   "source": [
    "While I recorded each of the ten examples using a good quality lavalier microphone in a quite environment, the first attempt to synthesise my voice produced poor results. The small amount of background noise and hiss was amplified and created strange ethereal artefacts in the recording. \n",
    "\n",
    "Converting the input voice samples from stereo to mono and using Audacity's noise removal filter (Effect > Noise Remover and Reduction) significantly improved the quality of the resulting custom voice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f227f34-105c-4903-9012-7791f4f5af11",
   "metadata": {},
   "source": [
    "Place a copy of the training samples in a training folder `./input-audio-samples/djh`\n",
    "\n",
    "Then reference this folder using a combination of the `-V` and `-v` options.\n",
    "\n",
    "I experimented with the seed value to find an custom voice that was similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73817e33-73da-4cd3-8f92-aa7d2e207969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2024-04-19 21:12:38,543] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n",
      "[2024-04-19 21:12:38,797] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.1, git-hash=unknown, git-branch=unknown\n",
      "[2024-04-19 21:12:38,797] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2024-04-19 21:12:38,797] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "Using /home/djh/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/djh/.cache/torch_extensions/py310_cu121/transformer_inference/build.ninja...\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.02948284149169922 seconds\n",
      "[2024-04-19 21:12:38,839] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000, 'invert_mask': True}\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/tortoise/utils/audio.py:17: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sampling_rate, data = read(full_path)\n",
      "Rendering djh_00 (1 of 1)...\n",
      "  Testing, testing, one two, one two three.\n",
      "Generating autoregressive samples..\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]------------------------------------------------------\n",
      "Free memory : 17.096436 (GigaBytes)  \n",
      "Total memory: 19.681274 (GigaBytes)  \n",
      "Requested memory: 5.375000 (GigaBytes) \n",
      "Setting maximum total tokens (input + output) to 1024 \n",
      "WorkSpace: 0x7fbe00000000 \n",
      "------------------------------------------------------\n",
      "100%|███████████████████████████████████████████| 16/16 [00:09<00:00,  1.65it/s]\n",
      "Computing best candidates using CLVP\n",
      "100%|███████████████████████████████████████████| 16/16 [00:05<00:00,  2.72it/s]\n",
      "Transforming autoregressive outputs into audio..\n",
      "100%|█████████████████████████████████████████| 400/400 [00:09<00:00, 43.45it/s]\n",
      "\n",
      "real\t0m39.743s\n",
      "user\t0m48.745s\n",
      "sys\t0m6.266s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 88 -p high_quality -V ./input-audio-samples -v djh -o djh-high-quality-fullp-deep-cache-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85af05-7ed8-4cf4-b6a0-681762e9784a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
