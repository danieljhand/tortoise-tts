{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e735e64-a1f8-4d14-b30a-368c323679fe",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook contains a record of my research and experimentation with [TorToiSe TTS](https://github.com/neonbjb/tortoise-tts), a Text To Speech Gen AI tool created by James Betker.\n",
    "\n",
    "Tortoise-TTS borrows two approaches for Gen AI more commonly used in text and image based Gen AI models. It uses auto-regression (auto: \"self\", regressus: \"to go back\") to create the next output token based on previous outputs right-shifted in time. The output speech sample is denoised using Denoising Diffusion Probabalistic Model (DDPM). These are two approaches more commonly associated with LLMs and Image generation.\n",
    "\n",
    "As it's name suggests, Tortoise was initially quite slow. High quality audio output would take tens of seconds to create a single word on a desktop graphics card. However, subsequent improvemets in caching of keys and values in the decoder, adoption of Microsofts DeepSpeed Pytorch optimisation library and precision options collectively improved performance by 5-6x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c442705-99c1-4418-ba88-4602849af69f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Toroise was installed in a virtual Python `v3.10` environment on a Ubuntu `22.04.3` LTS (GNU/Linux 5.15.0-91-generic x86_64) with `96GB` DDR5 ram, Intel Core `i5-13600K`, Samsung `NVME` storage and a `RTX 4000 NVIDIA` graphics card. Setting up of the environment is omitted from this notebook.\n",
    "\n",
    "## Installation of Tortoise-TTS\n",
    "Install `tortoise-tts` using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e30e7da-ab13-4719-91b1-07693643fbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/neonbjb/tortoise-tts\n",
      "  Cloning https://github.com/neonbjb/tortoise-tts to /tmp/pip-req-build-464da0zd\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neonbjb/tortoise-tts /tmp/pip-req-build-464da0zd\n",
      "  Resolved https://github.com/neonbjb/tortoise-tts to commit 1a3b014d5c5b14feaa416145004411a7ee2a3970\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting einops\n",
      "  Using cached einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "Collecting inflect\n",
      "  Using cached inflect-7.2.0-py3-none-any.whl (34 kB)\n",
      "Collecting librosa\n",
      "  Using cached librosa-0.10.1-py3-none-any.whl (253 kB)\n",
      "Collecting progressbar\n",
      "  Using cached progressbar-2.5.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rotary_embedding_torch\n",
      "  Using cached rotary_embedding_torch-0.5.3-py3-none-any.whl (5.3 kB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "Collecting tokenizers\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Collecting transformers==4.31.0\n",
      "  Using cached transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "Collecting unidecode\n",
      "  Using cached Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2024.4.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from transformers==4.31.0->tortoise-tts==3.0.0) (6.0.1)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from transformers==4.31.0->tortoise-tts==3.0.0) (2.31.0)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.13.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from transformers==4.31.0->tortoise-tts==3.0.0) (24.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Using cached huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "Collecting tokenizers\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Collecting typeguard>=4.0.1\n",
      "  Using cached typeguard-4.2.1-py3-none-any.whl (34 kB)\n",
      "Collecting more-itertools\n",
      "  Using cached more_itertools-10.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.10/site-packages (from inflect->tortoise-tts==3.0.0) (4.11.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./venv/lib/python3.10/site-packages (from librosa->tortoise-tts==3.0.0) (5.1.1)\n",
      "Collecting soundfile>=0.12.1\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
      "Collecting joblib>=0.14\n",
      "  Using cached joblib-1.4.0-py3-none-any.whl (301 kB)\n",
      "Collecting pooch>=1.0\n",
      "  Using cached pooch-1.8.1-py3-none-any.whl (62 kB)\n",
      "Collecting scikit-learn>=0.20.0\n",
      "  Using cached scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "Collecting numba>=0.51.0\n",
      "  Using cached numba-0.59.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "Collecting soxr>=0.3.2\n",
      "  Using cached soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting lazy-loader>=0.1\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Collecting msgpack>=1.0\n",
      "  Using cached msgpack-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n",
      "Collecting audioread>=2.1.9\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Collecting beartype\n",
      "  Downloading beartype-0.18.5-py3-none-any.whl (917 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m917.8/917.8 KB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch>=2.0\n",
      "  Using cached torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Collecting llvmlite<0.43,>=0.42.0dev0\n",
      "  Using cached llvmlite-0.42.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.8 MB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./venv/lib/python3.10/site-packages (from pooch>=1.0->librosa->tortoise-tts==3.0.0) (4.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->transformers==4.31.0->tortoise-tts==3.0.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->transformers==4.31.0->tortoise-tts==3.0.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->transformers==4.31.0->tortoise-tts==3.0.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->transformers==4.31.0->tortoise-tts==3.0.0) (2024.2.2)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in ./venv/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa->tortoise-tts==3.0.0) (1.16.0)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting triton==2.2.0\n",
      "  Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (3.1.3)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting nvidia-nvjitlink-cu12\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->tortoise-tts==3.0.0) (2.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch>=2.0->rotary_embedding_torch->tortoise-tts==3.0.0) (2.1.5)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using legacy 'setup.py install' for tortoise-tts, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for progressbar, since package 'wheel' is not installed.\n",
      "Installing collected packages: tokenizers, progressbar, mpmath, unidecode, typeguard, tqdm, threadpoolctl, sympy, safetensors, regex, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, msgpack, more-itertools, llvmlite, lazy-loader, joblib, fsspec, filelock, einops, beartype, audioread, triton, soxr, soundfile, scipy, pooch, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, inflect, huggingface-hub, transformers, scikit-learn, nvidia-cusolver-cu12, torch, librosa, rotary_embedding_torch, tortoise-tts\n",
      "  Running setup.py install for progressbar ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for tortoise-tts ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed audioread-3.0.1 beartype-0.18.5 einops-0.7.0 filelock-3.13.4 fsspec-2024.3.1 huggingface-hub-0.22.2 inflect-7.2.0 joblib-1.4.0 lazy-loader-0.4 librosa-0.10.1 llvmlite-0.42.0 more-itertools-10.2.0 mpmath-1.3.0 msgpack-1.0.8 networkx-3.3 numba-0.59.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pooch-1.8.1 progressbar-2.5 regex-2024.4.16 rotary_embedding_torch-0.5.3 safetensors-0.4.3 scikit-learn-1.4.2 scipy-1.13.0 soundfile-0.12.1 soxr-0.3.7 sympy-1.12 threadpoolctl-3.4.0 tokenizers-0.13.3 torch-2.2.2 tortoise-tts-3.0.0 tqdm-4.66.2 transformers-4.31.0 triton-2.2.0 typeguard-4.2.1 unidecode-1.3.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/neonbjb/tortoise-tts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6763851d-57b9-4ac1-970f-ff4fd935afca",
   "metadata": {},
   "source": [
    "Confirm the installation was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95bc58ef-5460-4408-a1cf-77ebaa391d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/djh/Development/python/tortoise-tts/venv/bin/tortoise_tts.py\", line 10, in <module>\n",
      "    import torchaudio\n",
      "ModuleNotFoundError: No module named 'torchaudio'\n"
     ]
    }
   ],
   "source": [
    "!tortoise_tts.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f1cdf-4ee1-4032-85ab-51b739295fc4",
   "metadata": {},
   "source": [
    "The `torchaudio` package appears to be missing from `requirements.txt`. Let's install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5836cbbe-1616-4620-951c-db59f4f8e63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.2.2-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: torch==2.2.2 in ./venv/lib/python3.10/site-packages (from torchaudio) (2.2.2)\n",
      "Requirement already satisfied: triton==2.2.0 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (11.4.5.107)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (1.12)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (3.13.4)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (3.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (2.19.3)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (3.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (4.11.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch==2.2.2->torchaudio) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchaudio) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch==2.2.2->torchaudio) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->torch==2.2.2->torchaudio) (1.3.0)\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-2.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "053614bb-854f-4760-bd4b-f55197826bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: tortoise_tts.py [-h] [-v, --voice VOICE] [-V, --voices-dir VOICES_DIR]\n",
      "                       [-p, --preset {ultra_fast,fast,standard,high_quality}]\n",
      "                       [-q, --quiet]\n",
      "                       (-l, --list-voices | -P, --play | -o, --output OUTPUT | -O, --output-dir OUTPUT_DIR)\n",
      "                       [--candidates CANDIDATES] [--regenerate REGENERATE]\n",
      "                       [--skip-existing] [--produce-debug-state] [--seed SEED]\n",
      "                       [--models-dir MODELS_DIR] [--text-split TEXT_SPLIT]\n",
      "                       [--disable-redaction] [--device DEVICE]\n",
      "                       [--batch-size BATCH_SIZE]\n",
      "                       [--num-autoregressive-samples NUM_AUTOREGRESSIVE_SAMPLES]\n",
      "                       [--temperature TEMPERATURE]\n",
      "                       [--length-penalty LENGTH_PENALTY]\n",
      "                       [--repetition-penalty REPETITION_PENALTY]\n",
      "                       [--top-p TOP_P] [--max-mel-tokens MAX_MEL_TOKENS]\n",
      "                       [--cvvp-amount CVVP_AMOUNT]\n",
      "                       [--diffusion-iterations DIFFUSION_ITERATIONS]\n",
      "                       [--cond-free COND_FREE] [--cond-free-k COND_FREE_K]\n",
      "                       [--diffusion-temperature DIFFUSION_TEMPERATURE]\n",
      "                       [text ...]\n",
      "\n",
      "TorToiSe is a text-to-speech program that is capable of synthesizing speech in\n",
      "multiple voices with realistic prosody and intonation.\n",
      "\n",
      "positional arguments:\n",
      "  text                  Text to speak. If omitted, text is read from stdin.\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --voice VOICE     Selects the voice to use for generation. Use the &\n",
      "                        character to join two voices together. Use a comma to\n",
      "                        perform inference on multiple voices. Set to \"all\" to\n",
      "                        use all available voices. Note that multiple voices\n",
      "                        require the --output-dir option to be set.\n",
      "  -V, --voices-dir VOICES_DIR\n",
      "                        Path to directory containing extra voices to be\n",
      "                        loaded. Use a comma to specify multiple directories.\n",
      "  -p, --preset {ultra_fast,fast,standard,high_quality}\n",
      "                        Which voice quality preset to use.\n",
      "  -q, --quiet           Suppress all output.\n",
      "  -l, --list-voices     List available voices and exit.\n",
      "  -P, --play            Play the audio (requires pydub).\n",
      "  -o, --output OUTPUT   Save the audio to a file.\n",
      "  -O, --output-dir OUTPUT_DIR\n",
      "                        Save the audio to a directory as individual segments.\n",
      "\n",
      "multi-output options (requires --output-dir):\n",
      "  --candidates CANDIDATES\n",
      "                        How many output candidates to produce per-voice. Note\n",
      "                        that only the first candidate is used in the combined\n",
      "                        output.\n",
      "  --regenerate REGENERATE\n",
      "                        Comma-separated list of clip numbers to re-generate.\n",
      "  --skip-existing       Set to skip re-generating existing clips.\n",
      "\n",
      "advanced options:\n",
      "  --produce-debug-state\n",
      "                        Whether or not to produce debug_states in current\n",
      "                        directory, which can aid in reproducing problems.\n",
      "  --seed SEED           Random seed which can be used to reproduce results.\n",
      "  --models-dir MODELS_DIR\n",
      "                        Where to find pretrained model checkpoints. Tortoise\n",
      "                        automatically downloads these to\n",
      "                        ~/.cache/tortoise/.models, so this should only be\n",
      "                        specified if you have custom checkpoints.\n",
      "  --text-split TEXT_SPLIT\n",
      "                        How big chunks to split the text into, in the format\n",
      "                        <desired_length>,<max_length>.\n",
      "  --disable-redaction   Normally text enclosed in brackets are automatically\n",
      "                        redacted from the spoken output (but are still\n",
      "                        rendered by the model), this can be used for prompt\n",
      "                        engineering. Set this to disable this behavior.\n",
      "  --device DEVICE       Device to use for inference.\n",
      "  --batch-size BATCH_SIZE\n",
      "                        Batch size to use for inference. If omitted, the batch\n",
      "                        size is set based on available GPU memory.\n",
      "\n",
      "tuning options (overrides preset settings):\n",
      "  --num-autoregressive-samples NUM_AUTOREGRESSIVE_SAMPLES\n",
      "                        Number of samples taken from the autoregressive model,\n",
      "                        all of which are filtered using CLVP. As TorToiSe is a\n",
      "                        probabilistic model, more samples means a higher\n",
      "                        probability of creating something \"great\".\n",
      "  --temperature TEMPERATURE\n",
      "                        The softmax temperature of the autoregressive model.\n",
      "  --length-penalty LENGTH_PENALTY\n",
      "                        A length penalty applied to the autoregressive\n",
      "                        decoder. Higher settings causes the model to produce\n",
      "                        more terse outputs.\n",
      "  --repetition-penalty REPETITION_PENALTY\n",
      "                        A penalty that prevents the autoregressive decoder\n",
      "                        from repeating itself during decoding. Can be used to\n",
      "                        reduce the incidence of long silences or \"uhhhhhhs\",\n",
      "                        etc.\n",
      "  --top-p TOP_P         P value used in nucleus sampling. 0 to 1. Lower values\n",
      "                        mean the decoder produces more \"likely\" (aka boring)\n",
      "                        outputs.\n",
      "  --max-mel-tokens MAX_MEL_TOKENS\n",
      "                        Restricts the output length. 1 to 600. Each unit is\n",
      "                        1/20 of a second.\n",
      "  --cvvp-amount CVVP_AMOUNT\n",
      "                        How much the CVVP model should influence the\n",
      "                        output.Increasing this can in some cases reduce the\n",
      "                        likelihood of multiple speakers.\n",
      "  --diffusion-iterations DIFFUSION_ITERATIONS\n",
      "                        Number of diffusion steps to perform. More steps means\n",
      "                        the network has more chances to iterativelyrefine the\n",
      "                        output, which should theoretically mean a higher\n",
      "                        quality output. Generally a value above 250 is not\n",
      "                        noticeably better, however.\n",
      "  --cond-free COND_FREE\n",
      "                        Whether or not to perform conditioning-free diffusion.\n",
      "                        Conditioning-free diffusion performs two forward\n",
      "                        passes for each diffusion step: one with the outputs\n",
      "                        of the autoregressive model and one with no\n",
      "                        conditioning priors. The output of the two is blended\n",
      "                        according to the cond_free_k value below.\n",
      "                        Conditioning-free diffusion is the real deal, and\n",
      "                        dramatically improves realism.\n",
      "  --cond-free-k COND_FREE_K\n",
      "                        Knob that determines how to balance the conditioning\n",
      "                        free signal with the conditioning-present signal.\n",
      "                        [0,inf]. As cond_free_k increases, the output becomes\n",
      "                        dominated by the conditioning-free signal. Formula is:\n",
      "                        output=cond_present_output*(cond_free_k+1)-\n",
      "                        cond_absenct_output*cond_free_k\n",
      "  --diffusion-temperature DIFFUSION_TEMPERATURE\n",
      "                        Controls the variance of the noise fed into the\n",
      "                        diffusion model. [0,1]. Values at 0 are the \"mean\"\n",
      "                        prediction of the diffusion network and will sound\n",
      "                        bland and smeared.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Read text using random voice and place it in a file:\n",
      "\n",
      "    tortoise_tts.py -o hello.wav \"Hello, how are you?\"\n",
      "\n",
      "Read text from stdin and play it using the tom voice:\n",
      "\n",
      "    echo \"Say it like you mean it!\" | tortoise_tts.py -P -v tom\n",
      "\n",
      "Read a text file using multiple voices and save the audio clips to a directory:\n",
      "\n",
      "    tortoise_tts.py -O /tmp/tts-results -v tom,emma <textfile.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tortoise_tts.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d1fae-bb43-455d-a0b6-bc5fe0cddd49",
   "metadata": {},
   "source": [
    "Tortoise-tts comes with approximately a dozen built-in voices. Let's see what voices we can choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a56f7049-3a77-4743-92ae-0a9f307bb1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: tortoise_tts.py [-h] [-v, --voice VOICE] [-V, --voices-dir VOICES_DIR]\n",
      "                       [-p, --preset {ultra_fast,fast,standard,high_quality}]\n",
      "                       [-q, --quiet]\n",
      "                       (-l, --list-voices | -P, --play | -o, --output OUTPUT | -O, --output-dir OUTPUT_DIR)\n",
      "                       [--candidates CANDIDATES] [--regenerate REGENERATE]\n",
      "                       [--skip-existing] [--produce-debug-state] [--seed SEED]\n",
      "                       [--models-dir MODELS_DIR] [--text-split TEXT_SPLIT]\n",
      "                       [--disable-redaction] [--device DEVICE]\n",
      "                       [--batch-size BATCH_SIZE]\n",
      "                       [--num-autoregressive-samples NUM_AUTOREGRESSIVE_SAMPLES]\n",
      "                       [--temperature TEMPERATURE]\n",
      "                       [--length-penalty LENGTH_PENALTY]\n",
      "                       [--repetition-penalty REPETITION_PENALTY]\n",
      "                       [--top-p TOP_P] [--max-mel-tokens MAX_MEL_TOKENS]\n",
      "                       [--cvvp-amount CVVP_AMOUNT]\n",
      "                       [--diffusion-iterations DIFFUSION_ITERATIONS]\n",
      "                       [--cond-free COND_FREE] [--cond-free-k COND_FREE_K]\n",
      "                       [--diffusion-temperature DIFFUSION_TEMPERATURE]\n",
      "                       [text ...]\n",
      "tortoise_tts.py: error: one of the arguments -l, --list-voices -P, --play -o, --output -O, --output-dir is required\n"
     ]
    }
   ],
   "source": [
    "!tortoise_tts.py --list-voices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d83381e-2894-49e2-8624-1306973dc124",
   "metadata": {},
   "source": [
    "That doesn't seem to be a valid option. Let's try using the `-v` option instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae994914-0631-4b25-9ab6-f383e04971d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angie\n",
      "applejack\n",
      "cond_latent_example\n",
      "daniel\n",
      "deniro\n",
      "emma\n",
      "freeman\n",
      "geralt\n",
      "halle\n",
      "jlaw\n",
      "lj\n",
      "mol\n",
      "myself\n",
      "pat\n",
      "pat2\n",
      "rainbow\n",
      "snakes\n",
      "tim_reynolds\n",
      "tom\n",
      "train_atkins\n",
      "train_daws\n",
      "train_dotrice\n",
      "train_dreams\n",
      "train_empire\n",
      "train_grace\n",
      "train_kennard\n",
      "train_lescault\n",
      "train_mouse\n",
      "weaver\n",
      "william\n"
     ]
    }
   ],
   "source": [
    "!tortoise_tts.py -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a17cc-6e70-4df3-9e2c-f3a01cf89935",
   "metadata": {},
   "source": [
    "That worked as expexted. Let's now create a test speech sample using the prerecorded voice `tom` with a quality preset of `ultra_fast`. This should be a baseline for the fastest, lowest fidelity output without using some of the subsequently added caching and library optimisation additions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba9961f-6fe5-401a-b337-536f48019b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Rendering tom_00 (1 of 1)...\n",
      "  Testing, testing, one two, one two three.\n",
      "Generating autoregressive samples..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.57s/it]\n",
      "Computing best candidates using CLVP\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.54it/s]\n",
      "Transforming autoregressive outputs into audio..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 84.06it/s]\n",
      "\n",
      "real\t0m23.414s\n",
      "user\t0m32.053s\n",
      "sys\t0m6.235s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 42 -p ultra_fast -v tom -o tom-ultra-fast-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfe9329-a7cb-4f3c-8d0b-e7c04133f8c0",
   "metadata": {},
   "source": [
    "The recording sound quality isn't great, but it is the `ultra_fast` preset option. Let's see how good the `high_quality` preset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e0d1e2-b8cb-4c7d-84bc-7cf8f518aacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Rendering tom_00 (1 of 1)...\n",
      "  Testing, testing, one two, one two three.\n",
      "Generating autoregressive samples..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [02:12<00:00,  8.29s/it]\n",
      "Computing best candidates using CLVP\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:05<00:00,  2.67it/s]\n",
      "Transforming autoregressive outputs into audio..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:09<00:00, 41.72it/s]\n",
      "\n",
      "real\t2m42.900s\n",
      "user\t2m51.587s\n",
      "sys\t0m6.390s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 42 -p high_quality -v tom -o tom-high-quality-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e532dd7d-719a-412e-8cb2-bea5bbe01fd7",
   "metadata": {},
   "source": [
    "The quality has improved but at the cost of 6x the GPU processing compared with using the `ultra_fast` preset. It requires approximately 30 seconds of GPU time per word to create a high fidelity output. As the author mentions, it's called ðŸ¢ tortoise for a good reason. Let's see if we can improve performance without losing too much quality.\n",
    "\n",
    "Let's first listen to the audio quality of the `standard` preset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ca33c8b-9734-4e59-ac3a-fabf76de0967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Rendering tom_00 (1 of 1)...\n",
      "  Testing, testing, one two, one two three.\n",
      "Generating autoregressive samples..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [02:12<00:00,  8.27s/it]\n",
      "Computing best candidates using CLVP\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:05<00:00,  2.68it/s]\n",
      "Transforming autoregressive outputs into audio..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:04<00:00, 41.63it/s]\n",
      "\n",
      "real\t2m37.722s\n",
      "user\t2m46.332s\n",
      "sys\t0m6.233s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 42 -p standard -v tom -o tom-standard-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109f114-d8fe-4eb9-afa8-3e61109088cd",
   "metadata": {},
   "source": [
    "I cannot hear a significant difference in the quality of the `standard` vs `high_quality` presents. The GPU time required by each is similar for this short test. Let's explore difference options starting with caching.\n",
    "\n",
    "## Improving Efficiency While Maintaining Performance\n",
    "\n",
    "\n",
    "### Key Value Caching\n",
    "Tortoise-tts includes support for `kv_caching`. Instead of recomputing the key matrix (all the previous context the model decoder should pay attention to) and value matrix (weighted sum over previous context), we can cache results. The k and v matrices don't change very much as we output new tokens. Refer to this [video](https://youtu.be/80bIUggRJf4?si=ceCOeyQlhFY9AiCD) for a helpful explanation.\n",
    "\n",
    "Unfortunately, the command line interface doesn't support a kv_cache input option so let's update `tortoise_tts.py` to enable it. Change line 209 of `venv/bin/tortoise_tts.py` to:\n",
    "\n",
    "```python\n",
    "tts = TextToSpeech(kv_cache=True, models_dir=args.models_dir, enable_redaction=not args.disable_redaction,\n",
    "\t\t\t\t   device=args.device, autoregressive_batch_size=args.batch_size)\n",
    "```\n",
    "\n",
    "and rerun the earlier test using the `high_quality` preset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ba8cd07-b41b-4410-88a5-5c26fcdb69e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Rendering tom_00 (1 of 1)...\n",
      "  Testing, testing, one two, one two three.\n",
      "Generating autoregressive samples..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:14<00:00,  1.08it/s]\n",
      "Computing best candidates using CLVP\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:05<00:00,  2.72it/s]\n",
      "Transforming autoregressive outputs into audio..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:09<00:00, 41.36it/s]\n",
      "\n",
      "real\t0m45.929s\n",
      "user\t0m53.813s\n",
      "sys\t0m5.994s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 42 -p high_quality -v tom -o tom-high-quality-kvcache-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c573a745-0514-46a2-adbc-4a80700cff94",
   "metadata": {},
   "source": [
    "Enabling `kv_cache` resulted in a significant improvement in performance. We've reduced the time taken by a factor of four but the audio fidelity sounds different. This is unexpected. I used the same `seed` value across tests so if `kv_cache` is indeed avoiding unnecessary computations of k and v values, I wouldn't expect a difference in the output wav file generated. It's difficult to objectively say that the new audio is worse without scientifically comparing the two segments, but qualitatively the latest recording doesn't sound as good.\n",
    "\n",
    "It looks like this is a known issue with the origial source code. It's been addressed in this derivative repo [tortoise-tts-fast](https://github.com/152334H/tortoise-tts-fast). Explore this later as the derivative code base uses a new sampler `dpm++2m`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6f6fe5-17e0-4d5b-a096-ff561a8ab1fb",
   "metadata": {},
   "source": [
    "### DeepSpeed Optimisation Library\n",
    "Tortoise-tts also supports the `DeepSpeed` library from Microsoft. One of the four innovation pillars of its development focusses on the inference of LLMs. Once again we'll need to enable its use by modifying `tortoise_tts.py`. Change line 209 of venv/bin/tortoise_tts.py to:\n",
    "\n",
    "```python\n",
    "tts = TextToSpeech(use_deepspeed=True, models_dir=args.models_dir, enable_redaction=not args.disable_redaction,\n",
    "                   device=args.device, autoregressive_batch_size=args.batch_size)\n",
    "```                   \n",
    "and rerun the earlier test using the high_quality preset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046e7d4-3571-4d93-bcbf-ab77138a79fb",
   "metadata": {},
   "source": [
    "> [!NOTE]  \n",
    "> I've removed the earlier setting of the argument `kv_cache` so we are only changing one variable at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004260ce-a4ff-41d5-9a53-c20b3a730874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/djh/Development/python/tortoise-tts/venv/bin/tortoise_tts.py\", line 209, in <module>\n",
      "    tts = TextToSpeech(use_deepspeed=True, models_dir=args.models_dir, enable_redaction=not args.disable_redaction,\n",
      "  File \"/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/tortoise/api.py\", line 218, in __init__\n",
      "    self.autoregressive.post_init_gpt2_config(use_deepspeed=use_deepspeed, kv_cache=kv_cache, half=self.half)\n",
      "  File \"/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/tortoise/models/autoregressive.py\", line 387, in post_init_gpt2_config\n",
      "    import deepspeed\n",
      "ModuleNotFoundError: No module named 'deepspeed'\n",
      "\n",
      "real\t0m8.923s\n",
      "user\t0m12.962s\n",
      "sys\t0m2.652s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 42 -p high_quality -v tom -o tom-high-quality-deepspeed-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd75adab-7116-4e49-a6cd-48f80a10766c",
   "metadata": {},
   "source": [
    "The `deepspeed` module is missing so we'll need to install it and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea47078d-0e24-442a-91ae-949f3a572a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepspeed\n",
      "  Using cached deepspeed-0.14.1-py3-none-any.whl\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from deepspeed) (24.0)\n",
      "Collecting pynvml\n",
      "  Using cached pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from deepspeed) (4.66.2)\n",
      "Collecting py-cpuinfo\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting hjson\n",
      "  Using cached hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.10/site-packages (from deepspeed) (5.9.8)\n",
      "Collecting ninja\n",
      "  Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.10/site-packages (from deepspeed) (2.2.2)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (from deepspeed) (1.26.4)\n",
      "Collecting pydantic\n",
      "  Using cached pydantic-2.7.0-py3-none-any.whl (407 kB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./venv/lib/python3.10/site-packages (from pydantic->deepspeed) (4.11.0)\n",
      "Collecting pydantic-core==2.18.1\n",
      "  Using cached pydantic_core-2.18.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (10.3.2.106)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (3.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (12.1.3.1)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (2.2.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (3.13.4)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch->deepspeed) (3.1.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch->deepspeed) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->torch->deepspeed) (1.3.0)\n",
      "Installing collected packages: py-cpuinfo, ninja, hjson, pynvml, pydantic-core, annotated-types, pydantic, deepspeed\n",
      "Successfully installed annotated-types-0.6.0 deepspeed-0.14.1 hjson-3.1.0 ninja-1.11.1.1 py-cpuinfo-9.0.0 pydantic-2.7.0 pydantic-core-2.18.1 pynvml-11.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "492bd41b-04c7-48c7-b49d-42886d8d0a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2024-04-19 12:53:38,898] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n",
      "[2024-04-19 12:53:39,147] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.1, git-hash=unknown, git-branch=unknown\n",
      "[2024-04-19 12:53:39,147] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2024-04-19 12:53:39,147] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "Using /home/djh/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/djh/.cache/torch_extensions/py310_cu121/transformer_inference/build.ninja...\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output pointwise_ops.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pointwise_ops.cu -o pointwise_ops.cuda.o \n",
      "[2/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output relu.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/relu.cu -o relu.cuda.o \n",
      "[3/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output dequantize.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/dequantize.cu -o dequantize.cuda.o \n",
      "[4/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output transform.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu -o transform.cuda.o \n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(38): warning #177-D: variable \"d0_stride\" was declared but never referenced\n",
      "      int d0_stride = hidden_dim * seq_length;\n",
      "          ^\n",
      "\n",
      "Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(66): warning #177-D: variable \"lane\" was declared but never referenced\n",
      "      int lane = d3 & 0x1f;\n",
      "          ^\n",
      "\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(109): warning #177-D: variable \"half_dim\" was declared but never referenced\n",
      "      unsigned half_dim = (rotary_dim << 3) >> 1;\n",
      "               ^\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int, float) [with T=__nv_bfloat16]\" at line 281\n",
      "\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(110): warning #177-D: variable \"d0_stride\" was declared but never referenced\n",
      "      int d0_stride = hidden_dim * seq_length;\n",
      "          ^\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int, float) [with T=__nv_bfloat16]\" at line 281\n",
      "\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(126): warning #177-D: variable \"vals_half\" was declared but never referenced\n",
      "      T2* vals_half = reinterpret_cast<T2*>(&vals_arr);\n",
      "          ^\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int, float) [with T=__nv_bfloat16]\" at line 281\n",
      "\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(127): warning #177-D: variable \"output_half\" was declared but never referenced\n",
      "      T2* output_half = reinterpret_cast<T2*>(&output_arr);\n",
      "          ^\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int, float) [with T=__nv_bfloat16]\" at line 281\n",
      "\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(144): warning #177-D: variable \"lane\" was declared but never referenced\n",
      "      int lane = d3 & 0x1f;\n",
      "          ^\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int, float) [with T=__nv_bfloat16]\" at line 281\n",
      "\n",
      "[5/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output gelu.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/gelu.cu -o gelu.cuda.o \n",
      "[6/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output apply_rotary_pos_emb.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu -o apply_rotary_pos_emb.cuda.o \n",
      "[7/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output rms_norm.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/rms_norm.cu -o rms_norm.cuda.o \n",
      "[8/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output softmax.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/softmax.cu -o softmax.cuda.o \n",
      "[9/11] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output layer_norm.cuda.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_89,code=sm_89 -gencode=arch=compute_89,code=compute_89 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/layer_norm.cu -o layer_norm.cuda.o \n",
      "[10/11] c++ -MMD -MF pt_binding.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/TH -isystem /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -DBF16_AVAILABLE -c /home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp -o pt_binding.o \n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of â€˜std::vector<at::Tensor> ds_softmax_context(at::Tensor&, at::Tensor&, int, bool, bool, int, int, float, bool, bool, int, bool, unsigned int, unsigned int, at::Tensor&, float) [with T = float]â€™:\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:2015:5:   required from here\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:541:50: warning: narrowing conversion of â€˜(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "  541 |                                      {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:541:50: warning: narrowing conversion of â€˜(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:542:41: warning: narrowing conversion of â€˜(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "  542 |                                       k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:542:41: warning: narrowing conversion of â€˜(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:550:38: warning: narrowing conversion of â€˜(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "  550 |                          {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:550:38: warning: narrowing conversion of â€˜(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:551:29: warning: narrowing conversion of â€˜(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "  551 |                           k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:551:29: warning: narrowing conversion of â€˜(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of â€˜std::vector<at::Tensor> ds_rms_mlp_gemm(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, float, at::Tensor&, at::Tensor&, bool, int, bool) [with T = float]â€™:\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:2015:5:   required from here\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1581:72: warning: narrowing conversion of â€˜(size_t)mlp_1_out_neuronsâ€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      " 1581 |         at::from_blob(intermediate_ptr, {input.size(0), input.size(1), mlp_1_out_neurons}, options);\n",
      "      |                                                                        ^~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1581:72: warning: narrowing conversion of â€˜mlp_1_out_neuronsâ€™ from â€˜const size_tâ€™ {aka â€˜const long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of â€˜std::vector<at::Tensor> ds_softmax_context(at::Tensor&, at::Tensor&, int, bool, bool, int, int, float, bool, bool, int, bool, unsigned int, unsigned int, at::Tensor&, float) [with T = __half]â€™:\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:2016:5:   required from here\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:541:50: warning: narrowing conversion of â€˜(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "  541 |                                      {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:541:50: warning: narrowing conversion of â€˜(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:542:41: warning: narrowing conversion of â€˜(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "  542 |                                       k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:542:41: warning: narrowing conversion of â€˜(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:550:38: warning: narrowing conversion of â€˜(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "  550 |                          {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:550:38: warning: narrowing conversion of â€˜(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:551:29: warning: narrowing conversion of â€˜(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "  551 |                           k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:551:29: warning: narrowing conversion of â€˜(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of â€˜std::vector<at::Tensor> ds_rms_mlp_gemm(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, float, at::Tensor&, at::Tensor&, bool, int, bool) [with T = __half]â€™:\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:2016:5:   required from here\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1581:72: warning: narrowing conversion of â€˜(size_t)mlp_1_out_neuronsâ€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      " 1581 |         at::from_blob(intermediate_ptr, {input.size(0), input.size(1), mlp_1_out_neurons}, options);\n",
      "      |                                                                        ^~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1581:72: warning: narrowing conversion of â€˜mlp_1_out_neuronsâ€™ from â€˜const size_tâ€™ {aka â€˜const long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of â€˜std::vector<at::Tensor> ds_softmax_context(at::Tensor&, at::Tensor&, int, bool, bool, int, int, float, bool, bool, int, bool, unsigned int, unsigned int, at::Tensor&, float) [with T = __nv_bfloat16]â€™:\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:2018:5:   required from here\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:541:50: warning: narrowing conversion of â€˜(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "  541 |                                      {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:541:50: warning: narrowing conversion of â€˜(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:542:41: warning: narrowing conversion of â€˜(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "  542 |                                       k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:542:41: warning: narrowing conversion of â€˜(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:550:38: warning: narrowing conversion of â€˜(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "  550 |                          {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:550:38: warning: narrowing conversion of â€˜(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:551:29: warning: narrowing conversion of â€˜(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "  551 |                           k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:551:29: warning: narrowing conversion of â€˜(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())â€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of â€˜std::vector<at::Tensor> ds_rms_mlp_gemm(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, float, at::Tensor&, at::Tensor&, bool, int, bool) [with T = __nv_bfloat16]â€™:\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:2018:5:   required from here\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1581:72: warning: narrowing conversion of â€˜(size_t)mlp_1_out_neuronsâ€™ from â€˜size_tâ€™ {aka â€˜long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      " 1581 |         at::from_blob(intermediate_ptr, {input.size(0), input.size(1), mlp_1_out_neurons}, options);\n",
      "      |                                                                        ^~~~~~~~~~~~~~~~~\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1581:72: warning: narrowing conversion of â€˜mlp_1_out_neuronsâ€™ from â€˜const size_tâ€™ {aka â€˜const long unsigned intâ€™} to â€˜long intâ€™ [-Wnarrowing]\n",
      "[11/11] c++ pt_binding.o gelu.cuda.o relu.cuda.o layer_norm.cuda.o rms_norm.cuda.o softmax.cuda.o dequantize.cuda.o apply_rotary_pos_emb.cuda.o transform.cuda.o pointwise_ops.cuda.o -shared -lcurand -L/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o transformer_inference.so\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 15.73634123802185 seconds\n",
      "[2024-04-19 12:53:54,895] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000, 'invert_mask': True}\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Rendering tom_00 (1 of 1)...\n",
      "  Testing, testing, one two, one two three.\n",
      "Generating autoregressive samples..\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]------------------------------------------------------\n",
      "Free memory : 17.104248 (GigaBytes)  \n",
      "Total memory: 19.681274 (GigaBytes)  \n",
      "Requested memory: 5.375000 (GigaBytes) \n",
      "Setting maximum total tokens (input + output) to 1024 \n",
      "WorkSpace: 0x7f4678000000 \n",
      "------------------------------------------------------\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:42<00:00,  2.64s/it]\n",
      "Computing best candidates using CLVP\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:05<00:00,  2.69it/s]\n",
      "Transforming autoregressive outputs into audio..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:09<00:00, 42.00it/s]\n",
      "\n",
      "real\t1m28.277s\n",
      "user\t1m59.131s\n",
      "sys\t0m9.289s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 42 -p high_quality -v tom -o tom-high-quality-deepspeed-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea1a61-f93f-4a40-8740-296ea012eae4",
   "metadata": {},
   "source": [
    "If this fails and complains of missing header files you may need to install the development python package for the version of python you are using in your vitual environment.\n",
    "\n",
    "```bash\n",
    "sudo apt-get install python3.10-dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4f849-87be-42c5-9978-8300438a567c",
   "metadata": {},
   "source": [
    "The deepspeed module reduced the GPU time by 50% with no audible reduction in the fidelity of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9056c688-4e90-42e1-ab65-1812fff63b67",
   "metadata": {},
   "source": [
    "The last performance option we'll explore is using half-precision 16 bit floating point numbers to store model weights/intermediate activations in the neural network. This will allow us to make more efficient use of the available GPU memory. Again we need to modify the code:\n",
    "\n",
    "```python\n",
    "tts = TextToSpeech(half=True, models_dir=args.models_dir, enable_redaction=not args.disable_redaction,\n",
    "                   device=args.device, autoregressive_batch_size=args.batch_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27d5e21c-8efe-462e-973a-3e8f79760ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Rendering tom_00 (1 of 1)...\n",
      "  Testing, testing, one two, one two three.\n",
      "Generating autoregressive samples..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:53<00:00,  3.34s/it]\n",
      "Computing best candidates using CLVP\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:02<00:00,  5.37it/s]\n",
      "Transforming autoregressive outputs into audio..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:09<00:00, 41.48it/s]\n",
      "\n",
      "real\t1m20.510s\n",
      "user\t1m29.539s\n",
      "sys\t0m5.881s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 42 -p high_quality -v tom -o tom-high-quality-halfp-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee807d-1d19-4f62-9b64-d52e56c4ac2c",
   "metadata": {},
   "source": [
    "Similar to using the deepspeed module, using half precision resulted in a 50% reduction in GPU time. While there's no obvious reduction in audio fidelity, I would expect a quantitative comparison to show there is a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aadb69-03e5-48fb-9e66-5e6dcbbdc144",
   "metadata": {},
   "source": [
    "### Combining Caching, DeepSpeed and Reduced Precision\n",
    "\n",
    "We can enable all three options at once, thereby hopefully resulting in a significant improvement in performance. Again we modify the code:\n",
    "\n",
    "```python\n",
    "tts = TextToSpeech(kv_cache=True, use_deepspeed=True, half=True, models_dir=args.models_dir, enable_redaction=not args.disable_redaction,\n",
    "                   device=args.device, autoregressive_batch_size=args.batch_size)\n",
    "```\n",
    "\n",
    "and rerun the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "364a24d7-efc5-44d5-9778-6689de791b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2024-04-19 13:19:09,928] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n",
      "[2024-04-19 13:19:10,177] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.1, git-hash=unknown, git-branch=unknown\n",
      "[2024-04-19 13:19:10,178] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2024-04-19 13:19:10,178] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "Using /home/djh/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/djh/.cache/torch_extensions/py310_cu121/transformer_inference/build.ninja...\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.029194355010986328 seconds\n",
      "[2024-04-19 13:19:10,258] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float16, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000, 'invert_mask': True}\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Rendering tom_00 (1 of 1)...\n",
      "  Testing, testing, one two, one two three.\n",
      "Generating autoregressive samples..\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]------------------------------------------------------\n",
      "Free memory : 17.815186 (GigaBytes)  \n",
      "Total memory: 19.681274 (GigaBytes)  \n",
      "Requested memory: 5.375000 (GigaBytes) \n",
      "Setting maximum total tokens (input + output) to 1024 \n",
      "WorkSpace: 0x7f235c000000 \n",
      "------------------------------------------------------\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:06<00:00,  2.43it/s]\n",
      "Computing best candidates using CLVP\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:02<00:00,  5.43it/s]\n",
      "Transforming autoregressive outputs into audio..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:08<00:00, 45.15it/s]\n",
      "\n",
      "real\t0m32.573s\n",
      "user\t0m42.285s\n",
      "sys\t0m5.633s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 42 -p high_quality -v tom -o tom-high-quality-halfp-deep-cache-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca3714-18cc-4001-bc37-593cd20eb4b9",
   "metadata": {},
   "source": [
    "We've reduced GPU time by approximately 85% without any notable reduction in the audible fidelity.\n",
    "\n",
    "I've included each of the synthetically generated text-to-speech audio files under the folder `./output-audio-samples` should you wish to listen and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e3b0c-e8f2-4acf-95e1-bc65e8833500",
   "metadata": {},
   "source": [
    "Let's now revist the earlier fork of the original repo that also reported that `kv_cache` changed the output recording. \n",
    "\n",
    "- https://github.com/neonbjb/tortoise-tts â†©\n",
    "    - https://github.com/152334H/tortoise-tts-fast/forks â†©\n",
    "        - https://github.com/manmay-nakhashi/tortoise-tts-fastest\n",
    "|\n",
    "Active development seems to have stopped, well at least that's what appears to be the case based on when the forks were last updated. The developer has choosen to develop on Python v3.8. While the code suggests that Python v3.10 should be a valid version, I found compatibility issues. I suspect this could result in non insignificant side project to get it running on v3.10 or later. Let's halt progress along this path and get back to the original repo `tortoise-tts`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcecf6b-deb7-4c7f-a682-6ffd3ec63c78",
   "metadata": {},
   "source": [
    "## Creating a Custom Voice\n",
    "\n",
    "Tortoise-tts supports custom voices. The process for creating a new voice appears to be quite simple, so let's do so. \n",
    "\n",
    "First we capture five to ten examples of the voice we wish to copy. Each recording should be no more than ten seconds long. I asked Google's Gemini for ten training texts. It provided the following suggestions:\n",
    "\n",
    "1. **Historical Fact:**Â The Great Wall of China is the longest man-made structure in the world, stretching over 21,000 kilometers. (9 seconds)\n",
    "2. **Instructional:**Â When crossing the street, look both ways before stepping off the curb. (8 seconds)\n",
    "3. **Technical Description:**Â A laptop computer is a portable personal computer designed for mobile use. (9 seconds)\n",
    "4. **News Report:**Â Local libraries will be offering a series of free workshops on creative writing throughout the summer. (10 seconds)\n",
    "5. **Restaurant Order:**Â I would like the chicken stir-fry with brown rice and a side salad, please. (8 seconds)\n",
    "6. **Travel Description:**Â The bustling marketplace was filled with vendors selling colorful fabrics and handcrafted souvenirs. (10 seconds)\n",
    "7. **Animal Fact:**Â The average lifespan of a domestic cat is around 15 years. (8 seconds)\n",
    "8. **Book Description:**Â This science fiction novel explores the concept of faster-than-light travel. (9 seconds)\n",
    "9. **Daily Routine:**Â I usually wake up at 7 am, exercise for 30 minutes, and then head to work. (9 seconds)\n",
    "10. **Movie Summary:**Â The heartwarming film follows a group of misfits who come together to achieve a common goal. (9 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef203cd-8cb6-4727-a04f-0dd7078c7aa6",
   "metadata": {},
   "source": [
    "While I recorded each of the ten examples using a good quality lavalier microphone in a quiet environment, the first attempt to synthesise my voice produced poor results. Small amounts of background noise and hiss were amplified and created strange ethereal artefacts in the recording. \n",
    "\n",
    "Converting the input voice samples from stereo to mono and using Audacity's noise removal filter (Effect > Noise Remover and Reduction) significantly improved the quality of the resulting custom voice. I feel there's still plenty of scope to improve this further but for now let's work what we have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f227f34-105c-4903-9012-7791f4f5af11",
   "metadata": {},
   "source": [
    "Place a copy of the training samples in a training folder `./input-audio-samples/djh`\n",
    "\n",
    "Then reference this folder using a combination of the `-V` and `-v` options.\n",
    "\n",
    "I experimented with the seed value to find an custom voice that was most similar to my voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73817e33-73da-4cd3-8f92-aa7d2e207969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tts...\n",
      "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2024-04-19 21:12:38,543] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n",
      "[2024-04-19 21:12:38,797] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.1, git-hash=unknown, git-branch=unknown\n",
      "[2024-04-19 21:12:38,797] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2024-04-19 21:12:38,797] [INFO] [logging.py:96:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "Using /home/djh/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/djh/.cache/torch_extensions/py310_cu121/transformer_inference/build.ninja...\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.02948284149169922 seconds\n",
      "[2024-04-19 21:12:38,839] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float32, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000, 'invert_mask': True}\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/home/djh/Development/python/tortoise-tts/venv/lib/python3.10/site-packages/tortoise/utils/audio.py:17: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sampling_rate, data = read(full_path)\n",
      "Rendering djh_00 (1 of 1)...\n",
      "  Testing, testing, one two, one two three.\n",
      "Generating autoregressive samples..\n",
      "  0%|                                                    | 0/16 [00:00<?, ?it/s]------------------------------------------------------\n",
      "Free memory : 17.096436 (GigaBytes)  \n",
      "Total memory: 19.681274 (GigaBytes)  \n",
      "Requested memory: 5.375000 (GigaBytes) \n",
      "Setting maximum total tokens (input + output) to 1024 \n",
      "WorkSpace: 0x7fbe00000000 \n",
      "------------------------------------------------------\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:09<00:00,  1.65it/s]\n",
      "Computing best candidates using CLVP\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:05<00:00,  2.72it/s]\n",
      "Transforming autoregressive outputs into audio..\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [00:09<00:00, 43.45it/s]\n",
      "\n",
      "real\t0m39.743s\n",
      "user\t0m48.745s\n",
      "sys\t0m6.266s\n"
     ]
    }
   ],
   "source": [
    "!time tortoise_tts.py --seed 88 -p high_quality -V ./input-audio-samples -v djh -o djh-high-quality-fullp-deep-cache-text01.wav \"Testing, testing, one two, one two three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5cb61f-e94f-42ca-905e-ea5a7cf068e8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "That concludes this short adventure into one of the seminal works and open source projects on Generative AI Test To Speech (TTS). The project is now over two years old but is still able to create realistic, high fidelity audio using only a small amount of training data. I could be a useful tool for TTS provided you invest the time to create high quality training data with a tone and cadence similar to the audio you want to create. It can also blend together multiple voices, which may allow us to inject mood into the spoken text.\n",
    "\n",
    "Text to Speech like so many other areas of Generative AI provide a rabbit's warren of options to explore. Tortoise-tts is one notable but aging project that still producing good results. In a future research session I may look into more recent open and closed source projects including Tacotron 2, coqui-TTS, Google's TTS and Amazon's BASE TTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21a0e9-e683-465a-8d32-c74a9bfb636d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
